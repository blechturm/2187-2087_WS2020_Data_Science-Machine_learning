---
title: "Unsupervised Learning Notebook"
subtitle: "Data Science and Machine Learning 2187 & 2087"
date: "12 2020"
author: Max Thomasberger
output: 
  html_document:
    toc: true
    toc_depth: 2
    #toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
bibliography: ./art/literature.bib
---


```{r setup, include=FALSE}

# deletes all objects from the environment
rm(list=ls())

knitr::opts_chunk$set(fig.align = 'center')

library(tidyverse)
library(here)
library(kableExtra)
library(summarytools)

source(here("02_Data_Wrangling","code","@@@_functions.R"))

```

# Intro

Note that we are using Tidyverse Version 1.3.0 in this notebook. Please update to this version if you have an older version installed. Some of the synthax presented here wont work with older versions (pivot_longer and pivot_wider). You will also have to install all the packages in the setup chunk above.

# Manual Implementation of K-means

Lets first remember how the K-means Algorith is defined. You can find the definitions of all the variables used on the slides.

## K-means Clustering Algorithm

Given a set of feature vectors $S_ n = \big \{ x^{(i)}| i = 1,...,n\big \}$ and the number of clusters  $K$ we can find cluster assignments  $C_1,\cdots,C_K$ and the representatives of each of the $K$ clusters $z_1,\cdots,z_K$:

1. Randomly select $z_1,\cdots,z_K$
2. Iterate until no change in cost

   + Given $z_1,\cdots,z_K$, assign each data point $x^{(i)}$ to the closest $z_j$, such that $\text {Cost}(z_1, ... z_ K) = \sum _{i=1}^{n} \min _{j=1,...,K} \left\|  x^{(i)} - z_ j \right\| ^2$
   + Given $C_1,\cdots,C_K$ find the best representatives $z_1,\cdots,z_K$, i.e. find $z_1,\cdots,z_K$ such that $\displaystyle z_ j=\operatorname {argmin}_{z} \sum _{i \in C_ j} \| x^{(i)} - z \| ^2$
   

## K-means Clustering in pseudo code for two clusters

1. Choose any two random coordinates, $z_1$ and $z_2$, on the scatter plot as initial
   cluster centers.
2. Calculate the distance of each data point in the scatter plot from coordinates $z_1$
   and $z_2$
3. Assign each data point to a cluster based on whether it is closer to $z_1$ or $z_2$
4. Find the mean coordinates of all points in each cluster and update the values of $z_1$
   and $z_2$ to those coordinates respectively.
5. Start again from Step 2 until the coordinates of $z_1$ and $z_2$ stop moving significantly,
   or after a certain pre-determined number of iterations of the process.

## Manual implementation

Now lets actually implement K-means together. We will start from the built in Iris dataset. See below a plot of two columns of the dataset and their species labels.

```{r, echo=T, out.width="70%",fig.align="center"}
iris_data <- iris

iris_data %>%
  ggplot() +
  aes(x=Sepal.Length, y= Sepal.Width, color=Species) +
  geom_point() +
  theme_bw()

```



Now lets drop these species labels and try to find our own three clusters using the K-means algorithm. We will initialize our three representatives $z_1,z_2,z_3$ randomly on the coordinate system. Note that I am not actually using a random initialization here to be consistent with the slides. 

You can try out other initialisations yourself but you might find out that sometimes the clusters aren't assigned because there are ties. How could those ties be resolved?

```{r}

z1 <- c(7,3)
z2 <- c(4.5,4)
z3 <- c(6.8,3.22)

iris_data %>%
  ggplot() +
  aes(x=Sepal.Length, y= Sepal.Width) +
  geom_point() +
  
  annotate(geom = "point", x = z1[1], 
           y = z1[2], shape = 5, 
           size = 3, color = "#F8766D") +
  
  annotate(geom = "point", x = z2[1], 
           y = z2[2], shape = 8, 
           size = 3, color = "#7CAE00") +
  
  annotate(geom = "point", x = z3[1], 
           y = z3[2], shape = 7, 
           size = 3, color = "#00BFC4") +
  
  theme_bw() +
  scale_x_continuous(limits=c(4,8)) +
  scale_y_continuous(limits=c(2,4.5)) +
  theme(legend.position = "none") 
```


Now lets calculate the euclidean square distance of each point in the dataset to our three initial representatives $z_1,z_2,z_3$. 

The formula for the eucledian squared distance is: $dist^2(x^{(i)},x^{(j)})= ||x^{(i)}-x^{(j)}||^2$. This distance measure generalizes to $d$ dimensions. But in our basic example we only have two dimensions so we will keep it simple:

The Euclidean distance between any two points is the square root of the sum
of squares of differences between the coordinates. Straight line distance between any two points (think of the pythagorean theorem)

* Eucledian distance two dimensions: $dist(p,q) = \sqrt{(p_1-q_1)^2 + (p_2 - q_2)^2}$. 
* Squared Euclidean distance is the sum of squares: $dist^2(p,q) = (p_1-q_1)^2 + (p_2 - q_2)^2$

Lets calculate the eucledian square distance between all the points in our data set and our three "random" representatives $z_1,z_2,z_3$. There are of course packages for this but we are doing manually for the sake of explicitness.

```{r}
iris_data <- iris_data %>%
  mutate(distance_to_C1 = (Sepal.Length - z1[1])^2 + (Sepal.Width - z1[2])^2,
         distance_to_C2 = (Sepal.Length - z2[1])^2 + (Sepal.Width - z2[2])^2,
         distance_to_C3 = (Sepal.Length - z3[1])^2 + (Sepal.Width - z3[2])^2)
```


Now lets find out which points are closest to each of the representatives.

```{r}

iris_data <- iris_data %>%
  
  # here we are doing the clustering assingments
  mutate(C1 = ifelse((distance_to_C1 < distance_to_C2 & 
                        distance_to_C1 < distance_to_C3), 
                     TRUE, FALSE),
         
         C2 = ifelse((distance_to_C1 > distance_to_C2 & 
                        distance_to_C3 > distance_to_C2), 
                     TRUE,FALSE),
         
         C3 = ifelse((distance_to_C3 < distance_to_C1 & 
                        distance_to_C3 < distance_to_C2), 
                     TRUE, FALSE)) %>%
  
  # here we are creating a factor variable for the 
  # clustering assignments (just for plotting convenience)
  
  mutate(cluster = case_when(C1 ~ "C1",
                             C2 ~ "C2",
                             C3 ~ "C3"))


iris_data %>%
    ggplot() +
    aes(x=Sepal.Length, y= Sepal.Width, color = cluster) +
    geom_point() +
  
    annotate(geom = "point", x = z1[1], 
             y = z1[2], shape = 5, 
             size = 3, color = "#F8766D") +
  
    annotate(geom = "point", x = z2[1], 
             y = z2[2], shape = 8, 
             size = 3, color = "#7CAE00") +
  
    annotate(geom = "point", x = z3[1],
             y = z3[2], shape = 7, 
             size = 3, color = "#00BFC4") +
  
    theme_bw() +
    scale_x_continuous(limits=c(4,8)) +
    scale_y_continuous(limits=c(2,4.5)) 

```


Yay! We just got our first cluster assignments. But as you can see our initial representatives $z_1,z_2,z_3$ do not seem to be very representative any more.

To fix this lets update our three initial representatives $z_1,z_2,z_3$. Remember that the optimal new representative (the one that minimizes the total clustering cost) is the centroid of the cluster $z_ j=\operatorname {argmin}_{z} \sum _{i \in C_ j} \| x^{(i)} - z \| ^2 = \frac{\sum _{i \in C_ j} x^{(i)}}{|C_ j|}$, where $|C_ j|$ is the size of the respective cluster. 

Lets translate this formula into plain english: it simply states that the optimal new representative (the one that minimizes the clustering cost) is the mean of all the points in the cluster. 


```{r}

# New representative for cluster 1:

z1[1] <- iris_data %>%
  filter(C1) %>%
  summarise(mean(Sepal.Length)) %>%
  as.numeric()

z1[2] <- iris_data %>%
  filter(C1) %>%
  summarise(mean(Sepal.Width)) %>%
  as.numeric()

# New representative for cluster 2:

z2[1] <- iris_data %>%
  filter(C2) %>%
  summarise(mean(Sepal.Length)) %>%
  as.numeric()

z2[2] <- iris_data %>%
  filter(C2) %>%
  summarise(mean(Sepal.Width)) %>%
  as.numeric()

# New representative for cluster 3:

z3[1] <- iris_data %>%
  filter(C3) %>%
  summarise(mean(Sepal.Length)) %>%
  as.numeric()

z3[2] <- iris_data %>%
  filter(C3) %>%
  summarise(mean(Sepal.Width)) %>%
  as.numeric()

# Now lets plot it

iris_data %>%
    ggplot() +
    aes(x=Sepal.Length, y= Sepal.Width, color = cluster) +
    geom_point() +
  
    annotate(geom = "point", x = z1[1], 
             y = z1[2], shape = 5, 
             size = 3, color = "#F8766D") +
  
    annotate(geom = "point", x = z2[1], 
             y = z2[2], shape = 8, 
             size = 3, color = "#7CAE00") +
  
    annotate(geom = "point", x = z3[1],
             y = z3[2], shape = 7, 
             size = 3, color = "#00BFC4") +
  
    theme_bw() +
    scale_x_continuous(limits=c(4,8)) +
    scale_y_continuous(limits=c(2,4.5)) 




```

Okay we are getting somewhere. Lets now assign the points to the new representatives.



```{r}


iris_data <- iris_data %>%
  
 # calculate distances to new cluster centers
  mutate(distance_to_C1 = (Sepal.Length - z1[1])^2 + (Sepal.Width - z1[2])^2,
         distance_to_C2 = (Sepal.Length - z2[1])^2 + (Sepal.Width - z2[2])^2,
         distance_to_C3 = (Sepal.Length - z3[1])^2 + (Sepal.Width - z3[2])^2) %>%
  
  # here we are doing the clustering assingments
  mutate(C1 = ifelse((distance_to_C1 < distance_to_C2 & 
                        distance_to_C1 < distance_to_C3), 
                     TRUE, FALSE),
         
         C2 = ifelse((distance_to_C1 > distance_to_C2 & 
                        distance_to_C3 > distance_to_C2), 
                     TRUE,FALSE),
         
         C3 = ifelse((distance_to_C3 < distance_to_C1 & 
                        distance_to_C3 < distance_to_C2), 
                     TRUE, FALSE)) %>%
  
  # here we are creating a factor variable for the 
  # clustering assignments (just for plotting convenience)
  
  mutate(cluster = case_when(C1 ~ "C1",
                             C2 ~ "C2",
                             C3 ~ "C3"))


iris_data %>%
    ggplot() +
    aes(x=Sepal.Length, y= Sepal.Width, color = cluster) +
    geom_point() +
  
    annotate(geom = "point", x = z1[1], 
             y = z1[2], shape = 5, 
             size = 3, color = "#F8766D") +
  
    annotate(geom = "point", x = z2[1], 
             y = z2[2], shape = 8, 
             size = 3, color = "#7CAE00") +
  
    annotate(geom = "point", x = z3[1],
             y = z3[2], shape = 7, 
             size = 3, color = "#00BFC4") +
  
    theme_bw() +
    scale_x_continuous(limits=c(4,8)) +
    scale_y_continuous(limits=c(2,4.5)) 

```


As you can see the the cluster assignments and the representatives will keep adjusting until convergence if we keep on doing these steps. Of course programming it like this is stupid and we should put everything in a loop that runs until the process reaches convergence. 

Below you can find the code inside a for loop for completeness but I do not recommend using it for actual projects because it was not programmed to be performant and flexible but rather to be educational. It is much better to use packages for actual projects because they have been tested "out in the wild" and have a lot of additional convenience features. But it is good practice to implement algos yourself if you truly want to understand them. We will have a look how to use some of these packages next.

Note that I do not check for convergence in this code chunk but rather let it run for 20 iterations which is enough to reach convergence in our example. 

How would you check for convergence?

```{r}

# Initialzation

z1<-c(7,3)
z2<-c(4.5,4)
z3<-c(6.8,3.22)

for(i in 1:20) {
  
  # Calculate distances to representatives and assign observations to clusters
  
  iris_data <- iris_data %>%
  
       # calculate distances to new cluster centers
        mutate(distance_to_C1 = (Sepal.Length - z1[1])^2 + (Sepal.Width - z1[2])^2,
               distance_to_C2 = (Sepal.Length - z2[1])^2 + (Sepal.Width - z2[2])^2,
               distance_to_C3 = (Sepal.Length - z3[1])^2 + (Sepal.Width - z3[2])^2) %>%
        
        # here we are doing the clustering assingments
        mutate(C1 = ifelse((distance_to_C1 < distance_to_C2 & 
                              distance_to_C1 < distance_to_C3), 
                           TRUE, FALSE),
               
               C2 = ifelse((distance_to_C1 > distance_to_C2 & 
                              distance_to_C3 > distance_to_C2), 
                           TRUE,FALSE),
               
               C3 = ifelse((distance_to_C3 < distance_to_C1 & 
                              distance_to_C3 < distance_to_C2), 
                           TRUE, FALSE)) %>%
        
        # here we are creating a factor variable for the 
        # clustering assignments (just for plotting convenience)
        
        mutate(cluster = case_when(C1 ~ "C1",
                                   C2 ~ "C2",
                                   C3 ~ "C3"))
      
      # Update representatives    
  
      # New representative for cluster 1:
      
      z1[1] <- iris_data %>%
        filter(C1) %>%
        summarise(mean(Sepal.Length)) %>%
        as.numeric()
      
      z1[2] <- iris_data %>%
        filter(C1) %>%
        summarise(mean(Sepal.Width)) %>%
        as.numeric()
      
      # New representative for cluster 2:
      
      z2[1] <- iris_data %>%
        filter(C2) %>%
        summarise(mean(Sepal.Length)) %>%
        as.numeric()
      
      z2[2] <- iris_data %>%
        filter(C2) %>%
        summarise(mean(Sepal.Width)) %>%
        as.numeric()
      
      # New representative for cluster 3:
      
      z3[1] <- iris_data %>%
        filter(C3) %>%
        summarise(mean(Sepal.Length)) %>%
        as.numeric()
      
      z3[2] <- iris_data %>%
        filter(C3) %>%
        summarise(mean(Sepal.Width)) %>%
        as.numeric()


}

# Plot Clustering

iris_data %>%
    ggplot() +
    aes(x=Sepal.Length, y= Sepal.Width, color = cluster) +
    geom_point() +
  
    annotate(geom = "point", x = z1[1], 
             y = z1[2], shape = 5, 
             size = 3, color = "#F8766D") +
  
    annotate(geom = "point", x = z2[1], 
             y = z2[2], shape = 8, 
             size = 3, color = "#7CAE00") +
  
    annotate(geom = "point", x = z3[1],
             y = z3[2], shape = 7, 
             size = 3, color = "#00BFC4") +
  
    theme_bw() +
    scale_x_continuous(limits=c(4,8)) +
    scale_y_continuous(limits=c(2,4.5)) 


```



# Example: Finding Teen Market Segments with K-means clustering

The data and the example are sourced from Chapter 9 of @Lantz2019. It has been adjusted to tidyverse synthax and expanded upon where necessary. 

The data represents a random sample of 30000 US high school stundents who had a social media profile on a well-known social network service in 2006. The data was sampled across four high school graduation years (2006 - 2009). The full text of the profiles were downloaded by a crawler. A text mining tool was used to divide the page content into words. From the top 500 words appearing across all pages, 36 words were chosen to represent five cateogries of interest: extracurricular activities, fashion, religion, romance and antisocial behavior. The final dataset indicates, for each person, how many times each word appeared in the respective profile.

## Exploratory Data-Analysis

Lets first have a look at the data. A useful package for this task is the summary tools package. It can produce automated summary tables with a lot of helpful stats.

As we can see the columns gender and age have a lot of missing data. And the age variable has unrealistic data at the tails, indicating that the respective user wanted to keep his/her real age hidden.

The dataset is skewed heavvily towards girls.

```{r message=FALSE, warning=FALSE}

# reading in the data set
teens <- read.csv(here("08_Unsupervised_Learning","data","snsdata.csv"), stringsAsFactors = TRUE)

# summary statistics
print(dfSummary(teens), method ="render")


```

## Naive imputations

Lets now do some naive imputations of the missing and implausible data.

* Why is this naive?
* How could this be improved?

```{r fig.width=5, fig.height=15,message=FALSE, warning=FALSE}

# wrangling and cleaning

teens_cleaned <- teens %>%
  mutate(age = ifelse(age >= 13 & age < 20, age, NA),
         female = ifelse(gender == "F",1,0),
         no_gender = ifelse(is.na(gender),1,0))

# Naive imputation of missing age variables

teens_cleaned <- teens_cleaned %>%
  group_by(gradyear) %>%
  mutate(ave_age = mean(age, na.rm = T)) %>%
  mutate(age = coalesce(age,ave_age)) %>%
  ungroup()

# check the summary results to ensure missing values are eliminated
summary(teens_cleaned$age)
```


## Min Max normalization

We are now almoste ready to train the model. But first we should normalize the data. Why?

$normalized_{minmax}= (value - min)/(max-min)$

* Most common way to normalize data.
* Minimum value of gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.
* Does not handle outliers very well.

## Z-score normalization

$normalized_{zscore}=\frac{value - \mu}{\sigma}$

* $\mu$ is the arithmetic mean of the data
* $\sigma$ is the standard deviation of the data
* Centers data at 0 an measures data in terms of standard deviations
* If value was below the mean the z-score is negative
* If value was above the mean the z-score is positive
* If unnormalized data had large $\sigma$, the normalized values will be closer to 0.
* Handles outliers better



```{r fig.width=5, fig.height=15,message=FALSE, warning=FALSE}
# create a z-score standardized data frame for easier interpretation

interests <- teens_cleaned %>%
  ungroup() %>%
  select(-c("gradyear","gender","age","friends", "female","no_gender","ave_age"))

interests_z <- teens_cleaned %>%
  ungroup() %>%
    select(-c("gradyear","gender","age","friends", "female","no_gender","ave_age")) %>%
  mutate_all(scale)

# compare the data before and after the transformation
summary(interests$basketball)
summary(interests_z$basketball)
```

## Clustering

Now we are ready to run k-means to find our clusters.

```{r fig.width=5, fig.height=15,message=FALSE, warning=FALSE}
# create the clusters using k-means
RNGversion("3.5.2") # use an older random number generator to match the book
set.seed(2345)

teen_clusters <- kmeans(interests_z, 5)

## Step 4: Evaluating model performance ----
# look at the size of the clusters
teen_clusters$size

# look at the cluster centers
teen_clusters$centers
```

Okay now lets find some interesting patterns in the data:

```{r fig.width=5, fig.height=15,message=FALSE, warning=FALSE}
teen_clusters$centers %>%
  as_tibble() %>%
  add_rownames(var ="cluster") %>%
  pivot_longer(cols=-cluster) %>%
  group_by(cluster) %>% 
  filter(value > 0) %>%
  View()

teen_clusters$centers %>%
  as_tibble() %>%
  add_rownames(var ="cluster") %>%
  pivot_longer(cols=-cluster) %>%
  mutate(positive = ifelse(value >0,"+","-")) %>%
  ggplot() +
  aes(x=reorder(name, value),y=value, fill=positive) +
  geom_col() +
  facet_wrap(cluster ~ .) +
  coord_flip()

```

Lets create some additional insights.

```{r fig.width=5, fig.height=15,message=FALSE, warning=FALSE}
# apply the cluster IDs to the original data frame
teens_cleaned$cluster <- teen_clusters$cluster

# mean age by cluster
teens_cleaned %>%
  group_by(cluster) %>%
  summarise(age = mean(age, na.rm = T))


# proportion of females by cluster
teens_cleaned %>%
  group_by(cluster) %>%
  summarise(female = mean(female, na.rm = T))


# mean number of friends by cluster

teens_cleaned %>%
  group_by(cluster) %>%
  summarise(friends = mean(friends, na.rm = T))

```

# References